## 前文提要

在前面两小节我们主要讲了导数以及复合函数的链式求导法则。这都是在为这一小节的内容做准备，这一小节我们开始讲解如何根据误差来调节神经网络的权重。

## 误差反向传播

我们前面讲了使用梯度下降的办法能让我们找到最小值点，而使用梯度下降思想的工具就是使用导数，还记得我们说的:
>想要到达最低点需要往斜率相反的方向走

接下来我们就具体讲解一下如何根据导数来更新权重。

### 反向传播

在神经网络中，从数据从输入层到隐藏层再到输出层传递我们称之为前向传播，而当我们得到误差以后，从输出层到隐藏层再到输入层更新权重时，我们称之为**反向传播**。

### 神经网络中的复合函数

要理解误差是如何反向传播的，我们必须清楚的知道前向传播是如何进行的。为了让大家能清晰的理解这一过程，我们先对符号的意思进行统一。

- $X$：表示输入数据矩阵
- $W^{(1)}$ :表示从输入层到隐藏层的权重矩阵。
- $B^{(1)}$ :表示输入层到隐藏层的偏置矩阵。
- $Z^{(2)}$ :表示神经网络第二层的输入矩阵，这里指的是隐藏层。
- $A^{(2)}$ :表示神经网络第二层的输出矩阵，这里指隐藏层的输出，也就是使用了激活函数后的值。
- $W^{(2)}$ :表示从隐藏层到输出层的权重矩阵。
- $B^{(2)}$ :表示隐藏层到输出层的的偏置矩阵。
- $Z^{(3)}$ :表示神经网络第三层的输入矩阵，这里指的是输出层。
- $A^{(3)}$ :表示神经网络第三层的输出矩阵，这里指输出层的输出，也就是使用了激活函数后的值。
- $L$ ：表示使用损失函数后的值。

为了方便讲解，我们假设有一个三层神经网络,隐藏层激活函数使用Sigmoid，输出层激活函数使用Softmax，损失函数使用交叉熵。我们来看看数据是如何从输入层到输出层最后计算出误差的。

1. $Z^{(2)} = W^{(1)} \cdot X + B^{(1)}$
2. $A^{(2)} = Sigmoid(Z^{(2)})$
3. $Z^{(3)} = W^{(2)} \cdot A^{(2)} + B^{(2)}$
4. $A^{(3)} = Softmax(Z^{(3)})$
5. $L = - \sum_{i=1}^n y_i logA^{(3)}$

以上就是数据从输入层传递到输出层，最后算出损失函数的完整步骤。从以上公式中我们发现，后一步的变量是前一步的计算结果。这不就是我们上一小节所介绍的复合函数吗？

那么我们想更新输入层到隐藏层的权重$W^{(1)}$，我们只需要计算$\frac{\partial L}{\partial W^{(1)}}$。我们就能知道当权重进行微小的调整时，损失会进行多大程度的变化。而想要求$\frac{\partial L}{\partial W^{(1)}}$，我们只需要使用复合函数的链式求导法则就能完成。

$$\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial A^{(3)}} \cdot \frac{\partial A^{(3)}}{\partial Z^{(3)}} \cdot \frac{\partial Z^{(3)}}{\partial A^{(2)}} \cdot \frac{\partial A^{(2)}}{\partial Z^{(2)}} \cdot \frac{\partial Z^{(2)}}{\partial W^{(1)}}$$

这样我们就能计算出$\frac{\partial L}{\partial W^{(1)}}$的值，得到这个值以后我们如何更新权重呢？

就要使用本小节开头的思想：

> 想要到达最低点需要往斜率相反的方向走

那么新的权重：

$W^{(1)}_{new} = W^{(1)}_{old} - \eta \cdot \frac{\partial L}{\partial W^{(1)}}$

其中$\eta$称为学习率。

### 学习率

学习率(Learning Rate)是神经网络训练过程中非常重要的一个超参数，它决定了每次权重更新的步长。

如果学习率设置过大，那么模型每次调整的步伐就会很大，就可能会导致在最优点附近反复横跳，导致模型无法收敛。

如果学习率设置过小，那么模型每次调整的步伐太小，可能会导致训练需要更多的迭代次数才能接近最优解，训练的周期变长。

在之后的学习中，我们会讲到如何动态调整学习率。初学者可以将学习率设置在0.001到0.01之间。这是深度学习任务中常见的学习率范围。

### 一个简单的例子

再了解完如何更新权重后，我们看一个简单的例子具体实践一下。

假设有一个简单的三层神经网络，输入层有2个节点，隐藏层有2个节点，输出层有2个节点。隐藏层的激活函数使用Sigmoid函数，输出层的激活函数使用Softmax函数，损失函数使用交叉熵。标签使用One-hot编码，具体数值如下：

1.结构：

- 输入层:2个节点($x_1$,$x_2$)
- 隐藏层:2个节点($h_1$,$h_2$)
- 输出层:2个节点($y_1$,$y_2$)

2.输入层到隐藏层

输入数据:

- $x_1 = 0.5$ ,$x_2 = 0.6$

权重：

- $w^{(1)}_{11} = 0.1$ ，$w^{(1)}_{12} = 0.2$
- $w^{(1)}_{21} = 0.3$ ，$w^{(1)}_{22} = 0.4$

偏置：
- $b^{1}_1 = 0.1 , b^{1}_2 = 0.2$

其中:

- $w^{(1)}_{11} $ 表示输入层第一个节点$x_1$到隐藏层第一个节点$h_1$的权重
- $w^{(1)}_{12} $ 表示输入层第二个节点$x_2$到隐藏层第一个节点$h_1$的权重
- $w^{(1)}_{21} $ 表示输入层第一个节点$x_1$到隐藏层第二个节点$h_2$的权重
- $w^{(1)}_{22} $ 表示输入层第二个节点$x_2$到隐藏层第二个节点$h_2$的权重
- $b^{1}_1$ 表示隐藏层第一个节点$h_1$的偏置。
- $b^{1}_2$ 表示隐藏层第二个节点$h_2$的偏置。

3.隐藏层到输出层

权重：
- $w^{(2)}_{11} = 0.7$ ，$w^{(2)}_{12} = 0.8$
- $w^{(2)}_{21} = 0.9$ ，$w^{(2)}_{22} = 1.0$

偏置：

- $b^{2}_1 = 0.1 , b^{2}_2 = 0.2$

其中:

- $w^{(2)}_{11} $ 表示隐藏层第一个节点$h_1$到输出层第一个节点$y_1$的权重
- $w^{(2)}_{12} $ 表示隐藏层第二个节点$h_2$到输出层第一个节点$y_1$的权重
- $w^{(2)}_{21} $ 表示隐藏层第一个节点$h_1$到输出层第二个节点$y_2$的权重
- $w^{(2)}_{22} $ 表示隐藏层第二个节点$h_2$到输出层第二个节点$y_2$的权重
- $b^{2}_1$ 表示输出层第一个节点$y_1$的偏置。
- $b^{2}_2$ 表示输出层第二个节点$y_2$的偏置。

4.标签

- $t_1 = 1$ , $t_2 = 0$

- $\eta = 0.01$

5.目标

进行一次前向传播，如何根据损失值对各权重进行一次调整。

6.如图：

![](https://files.mdnice.com/user/70350/0683d7a9-d57a-4fa7-8122-1842e0183044.png)


#### 前向传播

首先计算隐藏层的输入：

$z^2_1 = 0.1 \cdot 0.5+ 0.2 \cdot 0.6 + 0.1 = 0.05 + 0.12 + 0.1 = 0.27$

$z^2_2 = 0.3 \cdot 0.5+ 0.4 \cdot 0.6 + 0.2 = 0.15 + 0.24 + 0.2 = 0.59$

接着隐藏层使用激活函数Sigmoid：

$a^2_1 = \frac{1}{1+e^{-0.27}} \approx 0.567$

$a^2_2 = \frac{1}{1+e^{-0.59}} \approx 0.644$

再计算输出层的输入数据：

$z^3_1 = 0.7 \cdot 0.567+ 0.8 \cdot 0.644 + 0.1 = 0.397 + 0.515 + 0.1 = 1.012$

$z^3_2 = 0.9 \cdot 0.567+ 1 \cdot 0.644 + 0.2 = 0.51 + 0.644 + 0.2 = 1.354$


使用Softmax函数：

$a^3_1 = \frac{e^{1.012}}{e^{1.012}+e^{1.354}} \approx \frac{2.75}{2.75+3.87} \approx 0.415$

$a^3_2 = \frac{e^{1.354}}{e^{1.012}+e^{1.354}} \approx  \frac{3.87}{2.75+3.87} \approx 0.585$

使用交叉熵损失函数：

$L = -(t_1 \cdot log(a^3_1) + t2 \cdot log(a^3_2)) = -(1 \cdot log(0.415)) \approx 0.88$

#### 反向传播

在反向传播时，我们主要更新隐藏层到输出层的权重和输入层到隐藏层的权重。

隐藏层到输出层的权重：

$\frac{\partial L}{\partial W^2_{11}} = \frac{\partial L}{\partial a^3_1} \cdot \frac{\partial a^3_1}{\partial z^3_1} \cdot \frac{\partial z^3_1}{\partial W^2_{11}} $

我们在上一小节讲了，对Softmax和交叉熵函数组合而成的复合函数求导，其结果就等于输出值减去目标值，$a^3_1 - t_1$。所以$\frac{\partial L}{\partial Z^3_1} = a^3_1 - t_1$。

$$\frac{\partial L}{\partial w^2_{11}} = (a^3_1 - t_1) \cdot a^2_1 = (0.415 - 1) \cdot 0.567 \approx - 0.332$$

$$\frac{\partial L}{\partial w^2_{12}} = (a^3_1 - t_1) \cdot a^2_2 = (0.415 - 1) \cdot 0.644 \approx - 0.377$$

$$\frac{\partial L}{\partial w^2_{21}} = (a^3_2 - t_2) \cdot a^2_1 = (0.585 - 0) \cdot 0.567 \approx 0.332$$

$$\frac{\partial L}{\partial w^2_{22}} = (a^3_2 - t_2) \cdot a^2_2 = (0.585 - 0) \cdot 0.644 \approx 0.377$$

计算完隐藏层到输出层的权重以后，我们就能更新权重。 

$$ w^2_{11}(new) = w^2_{11}(old) - \eta \cdot \frac{\partial L}{\partial w^2_{11}} = 0.7 - 0.01 \cdot (-0.332) \approx 0.703 $$

$$ w^2_{12}(new) = w^2_{12}(old) - \eta \cdot \frac{\partial L}{\partial w^2_{12}} = 0.8 - 0.01 \cdot (- 0.377) \approx 0.804 $$

$$ w^2_{21}(new) = w^2_{21}(old) - \eta \cdot \frac{\partial L}{\partial w^2_{21}} = 0.9 - 0.01 \cdot 0.332 \approx 0.897 $$

$$ w^2_{22}(new) = w^2_{22}(old) - \eta \cdot \frac{\partial L}{\partial w^2_{22}} = 1 - 0.01 \cdot 0.377 \approx 0.996 $$

这样隐藏层到输出层的权重就更新完毕。接下来继续反向传播，更新输入层到隐藏层的权重。

在这里有一点需要注意，我们在求$\frac{\partial L}{\partial a^2_1}$时，由于$z^3_1$和$z^2_2$的计算都依赖于$ a^2_1$,所以这里要分别求偏导再相加。

$$\frac{\partial L}{\partial a^2_1} = \frac{\partial L}{\partial a^3_1} \cdot \frac{\partial a^3_1}{\partial z^3_1} \cdot \frac{\partial z^3_1}{\partial a^2_1} + \frac{\partial L}{\partial a^3_2} \cdot \frac{\partial a^3_2}{\partial z^3_2} \cdot \frac{\partial z^3_2}{\partial a^2_1} = (0.415-1) \cdot 0.7 + (0.585 - 0) \cdot 0.9 = -0.41 + 0.526 =0.116$$

而隐藏层的激活函数是Sigmoid,我们之前讲过Sigmoid的导数公式，所以：

$$\frac{\partial a^2_1}{\partial z^2_1} = a^2_1 \cdot (1-  a^2_1) = 0.567 \cdot (1- 0.567)\approx 0.246$$

最后计算$\frac{\partial L}{\partial w^1_{11}}$：

$$\frac{\partial L}{\partial w^1_{11}} = \frac{\partial L}{\partial a^2_1} \cdot \frac{\partial a^2_1}{\partial z^2_1} \cdot \frac{\partial z^2_1}{\partial w^1_{11}} = 0.116 \cdot 0.246 \cdot 0.5 \approx 0.014$$

按照以上步骤分别求出输入层到隐藏层的权重梯度：

$$\frac{\partial L}{\partial w^1_{12}} = \frac{\partial L}{\partial a^2_1} \cdot \frac{\partial a^2_1}{\partial z^2_1} \cdot \frac{\partial z^2_1}{\partial w^1_{12}} = 0.116 \cdot 0.246 \cdot 0.6 \approx 0.017$$

$$\frac{\partial L}{\partial w^1_{21}} = \frac{\partial L}{\partial a^2_2} \cdot \frac{\partial a^2_2}{\partial z^2_2} \cdot \frac{\partial z^2_2}{\partial w^1_{21}} = 0.117 \cdot 0.229 \cdot 0.5 \approx 0.013$$

$$\frac{\partial L}{\partial w^1_{22}} = \frac{\partial L}{\partial a^2_2} \cdot \frac{\partial a^2_2}{\partial z^2_2} \cdot \frac{\partial z^2_2}{\partial w^1_{22}} = 0.117 \cdot 0.229 \cdot 0.6 \approx 0.016$$

再更新输入层到隐藏层的权重：

$$ w^1_{11}(new) = w^1_{11}(old) - \eta \cdot \frac{\partial L}{\partial w^1_{11}} = 0.1 - 0.01 \cdot 0.014 = 0.09986 $$

$$ w^1_{12}(new) = w^1_{12}(old) - \eta \cdot \frac{\partial L}{\partial w^1_{12}} = 0.2 - 0.01 \cdot 0.017 = 0.19983 $$

$$ w^1_{21}(new) = w^1_{21}(old) - \eta \cdot \frac{\partial L}{\partial w^1_{21}} = 0.3 - 0.01 \cdot 0.013 = 0.29987 $$

$$ w^1_{22}(new) = w^1_{22}(old) - \eta \cdot \frac{\partial L}{\partial w^1_{22}} = 0.4 - 0.01 \cdot 0.016 = 0.39984 $$

到此为止，我们就完成了一轮前向传播与反向传播，我们可以看到更新后的权重都有不同程度上的调整。知道下一批数据再次进入神经网络后又会更新权重，直到训练完成。

## 总结

本小节讲解了如何通过反向传播来更新神经网络的权重，并且我们给出了一个具体的例子来对每一步进行讲解，本节内容有点烦琐，希望大家可以静下心来一步步的跟着进行推演。