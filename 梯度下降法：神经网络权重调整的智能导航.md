## 前文提要
在上一小节中，我们讲解了为什么要使用损失函数，以及介绍了两种常用的损失函数。损失函数可以指导我们优化神经网络的参数，那么损失函数是如何指导我们进行优化的呢？

## 梯度下降
我们来考虑以下一个场景，现在有一个3层的神经网络，每一层都有3个节点。请问如何调整输入层的第一个节点到隐藏层的第二个节点之间的链路权重，使得输出层的第三个节点的输出增加0.5？

事实上我们很难做到这一点，即使我们碰运气做到了，也会由于需要调整另外一个权重而破坏。以上问题其实是在表明，我们应该如何调整权重，进而优化神经网络？

在回答这个问题之前，我们先来想象一个场景，你在一个伸手不见五指的夜晚，独自一人在连绵不绝的群山当中，这些山岭地势非常的复杂，有山峰有波谷。你目前处于一个山坡上，你现在急需到谷底，你没有任何的地图，只有一把手电筒，你能通过手电筒来观察你附近的地形。如果你想快速的到达谷地，你应该怎么做？

![](https://files.mdnice.com/user/70350/30bb491c-18f6-4304-bdba-55144067be3f.png)

由于你只能看清周围的地形，所以你大概率第一反应是每次都往附近最低点走去。这个方法是正确的，虽然每次都选择最低点有可能走入局部最低点，但是你也能经过多次试错达到谷地。

这有点像计算机中的贪心算法，每次都选择局部最优解，虽然最后结果未必是全局最优的，但是这个方法有效。

在数学上，这种方法称为梯度下降（gradient descent）,梯度指地面的坡度，你走的方向是坡度最陡的向下方向。

那么梯度下降给了我们什么启发呢？我们调整权重的目的是为了让输出结果更接近于目标值，那么我们可以观察损失函数，我们让损失函数的值达到最小值也就接近了目标值。借用梯度下降的思维，我们只需要让损失函数的值一步一步的变小，那么神经网络的输出误差就一步步变小，从而接近目标结果。

我们使用一个简单的二次函数来模拟这种情况，假设有一个简单的二次函数

$y = (x  - 6)^2 + 1$

以下是该函数的图形，我们希望可以找到一个x使得y最小化，用数学的语言就是找到函数的最小值。
![](https://files.mdnice.com/user/70350/92e2a6d7-ab2f-46ba-9fcb-192e3819b53b.png)

从图中我们可以看见，使用梯度下降的思维，一小步一小步的往最小值方向移动。那么我们怎么知道哪个方向是最小值方向呢？在上图中，我们可以看到斜率为负，我们需要沿着x轴向右移动才能到达最低点。

如果现在处于函数右边，那么斜率为正，我们需要沿着x轴向左移动才能到达最低点。
![](https://files.mdnice.com/user/70350/ccb6c9ca-8800-4ce9-a666-91244d9b6b5a.png)

而当我们处于最小值点时，斜率为0。
![](https://files.mdnice.com/user/70350/e549ae84-9b47-4c02-8210-c08968868b47.png)

从以上的情况我们似乎可以得出如下结论：想要到达最低点需要往斜率相反的方向增加x值。当斜率为正时，我们需要往斜率为负的方向增加x值，这里可以理解为减少x值。当斜率为负时，我们需要往斜率为正的方向增加x值。

在这里需要注意，很多初学者会想为什么不直接使用代数计算出这个二次函数的最小值？而需要这么麻烦的移动呢？因为我们是通过这个简单的二次函数来进行模拟复杂的情况。从而找到一种不直接计算复杂函数就能找到最小值的通用方法。

比如当函数有两个自变量时，该函数可能非常复杂，想要通过计算来取最小值效率会很低。采用梯度下降法，可以更有效且更通用的找到最小值。
![](https://files.mdnice.com/user/70350/1639bae1-5798-494a-83d2-46c956b65a95.png)

但是从上图中，我们发现了新的情况。按照梯度下降法，每次往周围最低点位置进行小步移动，很有可能到达的不是全局最小值点，而是局部最小值点。

并且也会出现到达鞍点的情况，所谓鞍点就是在多维空间中，函数在该点处的梯度为0，而且该点还也不是局部最小值。你可以想象一下马鞍的形状，在某些方向上（例如从前到后）函数是凸的（形状像向上的弯曲），而在其他方向（例如从左到右）函数是凹的（形状像向下的弯曲）。在马鞍的中央点，尽管在该点上斜率为零，但你可以通过不同的方向获得不同的高度变化。这就是鞍点的本质特征。

![](https://files.mdnice.com/user/70350/6a4e0018-3e06-43ab-b4c0-8bc3de1ab9c6.png)

所以为了避免到达局部最小值和鞍点，我们可以从山上的不同点开始，多次利用梯度下降法，确保并不总是到达局部最小值点或者鞍点。不同的起点意味着不同的起始参数，在神经网络的场景里意味着不同的起始权重。还有一些其他的方法，我们将在后面优化神经网络的章节中进行讲解。


## 总结

本小节讲解了反向传播中最重要的思想---梯度下降法。理解梯度下降法是理解整个反向传播更新权重的重要环节。我们下一小节会继续更深入的讲解梯度下降法。