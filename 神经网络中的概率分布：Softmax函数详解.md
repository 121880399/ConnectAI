## 前文提要
上一小节我讲解了如何使用矩阵乘法来简化隐藏层的输入数据计算。这种矩阵乘法的应用可以推广到所有层的输入数据计算。未来再学习过程中我们将频繁的使用矩阵乘法。

## 简单例子

在开始本小节内容之前，我们先通过一个例子来巩固之前学习的内容。

现在有一个神经网络，输入层有3个节点，隐藏层也有3个节点，输出层有2个节点。输入数据和权重如图所示。我们希望计算输出层的输入数据。

![](https://files.mdnice.com/user/70350/ac148d66-a076-4f38-b32c-f94bd5205e27.png)


根据我们之前学的内容，我们知道要计算输出层的输入数据必须先知道隐藏层的输出结果。而隐藏层的输出结果又依赖于隐藏层的输入数据和激活函数。

隐藏层输入数据的计算：

$z = w · x + b$ =
$\begin{pmatrix}
0.9 & 0.4 & 0.6 \\ 
0.5 & 0.3 & 0.7 \\
0.2 & 0.1 & 0.4 \\
\end{pmatrix}\\$
·
$\begin{pmatrix}
0.5 \\ 
0.1 \\
0.4 \\
\end{pmatrix}\\$
+
$\begin{pmatrix}
0.1 \\ 
0.4 \\
0.11 \\
\end{pmatrix}\\$
= $\begin{pmatrix}
0.83 \\ 
0.96 \\
0.38 \\
\end{pmatrix}\\$

其中$b$为偏置。

隐藏层的激活函数使用Sigmoid函数。那么隐藏层的输出数据为：

$out = \sigma(z) = sigmoid\\$
$\begin{pmatrix}
0.83 \\ 
0.96 \\
0.38 \\
\end{pmatrix}=\\$
$\begin{pmatrix}
0.696\\
0.723\\
0.594
\end{pmatrix}$

还记得Sigmoid函数吗？

$\sigma(x) = \frac{1}{1+e^{-x}}\$

对于隐藏层第一个节点，将它的输入数据给到激活函数就是：

$\sigma(x) = \frac{1}{1+e^{-0.83}}\ = \frac{1}{1+0.436}\ = 0.696$ 

我们拿到隐藏层的输出数据以后，我们就可以计算输出层的输入数据了：

$z = w · out + b$ =
$\begin{pmatrix}
0.1 & 0.5 & 0.8 \\ 
0.4 & 0.2 & 0.1\\
\end{pmatrix}\\$
·
$\begin{pmatrix}
0.696\\
0.723\\
0.594
\end{pmatrix}\\$
+
$\begin{pmatrix}
0.4\\
0.7\\
\end{pmatrix}\\$
= $\begin{pmatrix}
1.306\\
1.183\\
\end{pmatrix}\\$

到了这里我们就算出了输出层所需要的输入数据。形状是一个2行1列的矩阵。

## 输出层的激活函数

在上面例子中，我们计算了输出层的输入数据，按照隐藏层的规则，这时候需要使用激活函数了，那么输出层的激活函数跟隐藏层的有什么不一样呢？

在神经网络中，输出层的激活函数通常与隐藏层的激活函数不一样，这是因为输出层的激活函数需要根据具体的任务来选择，以确保网络输出能够适应特定的任务需求。

**1.分类问题**

分类问题指的是数据属于哪一个类别的问题。比如区分图像中的人是男性还是女性。

二元分类：输出层通常使用 Sigmoid 激活函数，因为它将输出值限制在0到1之间，可以解释为概率值。

多元分类：输出层通常使用 Softmax 激活函数，它不仅将输出值限制在0到1之间，还确保所有输出值的和为1，这样可以表示各个类别的概率。

**2.回归问题**

回归问题指的是根据某个输入预测一个数值的问题。比如图像中的人预测其体重。

输出层通常使用 线性激活函数（即无激活函数），这样输出值可以是任意实数，适合预测连续值。


## Softmax函数

由于Sigmoid函数我们已经讲解过了，今天就来讲解一下Softmax函数。

**数学定义**

给定一个n维输入向量$z=[z_1,z_2,...,z_n]$,Softmax函数的输出$y = [y_1,y_2,...,y_n]$定义如下：

$y_i = \frac{e^{z_i}}{\sum_{j=1}^ne^{z_j} } $

其中e是自然对数的底(大约等于2.71828),是指数函数的基础。

$z_i$是第i个神经元的加权输入。

分子$e^{z_i}$是第i个神经元的指数函数。

分母$\sum_{j=1}^ne^{z_j}$是所有神经元的指数函数之和。

看数学定义感觉Softmax函数很复杂。其实说人话就是：输出层某个节点使用激活函数后的值就等于该节点的输入数据的指数函数除以输出层所有节点的指数函数之和。

## 特性

1.Softmax函数输出的每个元素都是在(0,1)之间的实数。

2.Softmax函数输出的所有元素的和为1。

代码实现：
```python
 def softmax(z):
   exp_z = np.exp(z)
   sum = np.sum(exp_z)
   out  = exp_z / sum
   
   return out

```

以上代码实现很简单，softmax函数接收一个向量。然后用numpy的exp函数来计算每个输入数据的指数函数。再将所有数据的指数行数就和。最后将每个输入数据的指数函数除以总和。

## 注意

上面的代码虽然描述了softmax函数，但是在计算机运行的时候存在一定的问题。就是有可能存在溢出。Softmax函数要进行指数运算，当指数函数很大时，比如$e^{100}$，再进行求和。最后进行除法运算，结果会“不确定”。这是因为在编写代码时，变量表示的范围是有限的，超出这个范围就会发生溢出。

怎么办？我们可以通过让输入数据统一减去输入数据中的最大值来降低指数函数的次数。例如有如下一组数据（998，1000,888），通过将减去最大值1000得到(-2,0,-112)。这时再来进行指数运算。

修改后的代码:
```python
import numpy as np

def softmax(z):
    """
    计算输入向量 z 的 Softmax 值
    
    参数:
    z (array-like): 输入向量，可以是一维或二维数组
    
    返回:
    array: Softmax 变换后的向量，保持输入的形状
    """
    # 确保输入是 NumPy 数组
    z = np.array(z)

    # 减去最大值，提升数值稳定性
    z_max = np.max(z)

    # 计算指数函数
    z_exp = np.exp(z - z_max)

    # 计算 softmax
    softmax_output = z_exp / np.sum(z_exp)

    return softmax_output

```

修改完的代码中，首先确保了输入的数组是Numpy数组。

然后计算输入数据中的最大值。再用输入数据分别减去最大值。最后进行指数运算和除法运算。


## 使用Softmax函数

现在我们已经实现了Softmax函数，那么让我们用本小节开头的例子，将隐藏层的输入数据代入。

$\sigma(z) = Softmax\\$
$\begin{pmatrix}
1.306\\
1.183\\
\end{pmatrix}=\\$
$\begin{pmatrix}
0.5308959\\
0.4691041\\
\end{pmatrix}\\$

从结果可以看到，Softmax函数将输入向量转换成了一个概率分布，每个值的范围都在(0,1)之间，且它们的总和为1。

在实践中，在学习阶段我们通常会使用softmax函数，但是在推理阶段往往会把softmax函数省略，因为即便使用了softmax函数，神经元之间的大小关系也不会改变，而且由于指数函数的运算需要一定的计算机算力，所以在推理阶段会把softmax函数省略。

## 完整代码

我们将这一小节的例子完整实现代码提供给大家：

```python
import numpy as np

def sigmoid(x):
    return 1/(1+np.exp(-x))

def softmax(z):
    """
    计算输入向量 z 的 Softmax 值
    
    参数:
    z (array-like): 输入向量，可以是一维或二维数组
    
    返回:
    array: Softmax 变换后的向量，保持输入的形状
    """
    # 确保输入是 NumPy 数组
    z = np.array(z)

    # 减去最大值，提升数值稳定性
    z_max = np.max(z)

    # 计算指数函数
    z_exp = np.exp(z - z_max)

    # 计算 softmax
    softmax_output = z_exp / np.sum(z_exp)

    return softmax_output

#隐藏层偏置
b = np.array([[0.1],[0.4],[0.11]])
#输入层数据
x = np.array([[0.5],[0.1],[0.4]])
#输出层到隐藏层的权重
w = np.array([[0.9,0.4,0.6],[0.5,0.3,0.7],[0.2,0.1,0.4]])

#隐藏层的输入数据
out = np.dot(w,x) + b

print(out)

#隐藏层使用激活函数
out = sigmoid(out)

print(out)

#隐藏层到输出层的权重
w2 = np.array([[0.1,0.5,0.8],[0.4,0.2,0.1]])

#输出层的偏置
b2 = np.array([[0.4],[0.7]])

#输出层的输入数据
z = np.dot(w2,out) + b2

print(z)

#输出层使用激活函数
out2 = softmax(z)

print("softmax:" + str(out2))




```

## 总结

本小节从一个简单的例子开始，将之前学习的内容串联了起来，接着讲解了输出层的激活函数Softmax。