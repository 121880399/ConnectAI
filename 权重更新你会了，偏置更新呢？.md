## 前文提要
上一小节我们介绍了误差反向传播的原理，以及如何更新权重，但是我们没有讲解偏置的更新，在这一小节里，我们将讲述偏置的更新，以及用代码实现一个简单的神经网络。

## 偏置更新

在误差反向传播的过程中，我们不仅仅要更新神经网络的权重，还需要更新偏置。偏置的更新规则与权重的更新规则类似，都是通过反向传播中的梯度下降法进行更新。偏置是每个神经元的一个独立参数，不依赖于输入，而是直接影响到输出。因此，偏置的更新仅依赖于反向传播过程中计算出的误差，并按照学习率的比例进行更新。

#### 偏置更新的数学公式

对于给定的偏置b，它的更新规则如下：

$$b_{new} = b_{old} - \eta \cdot \frac{\partial L}{\partial b}$$

其中：

- $b_{new}$是更新后的偏置。
- $b_{old}$是当前的偏置值。
- $\eta$是学习率。
- $\frac{\partial L}{\partial b}$是损失函数L对偏置b的偏导数。

#### 具体步骤

我们还是以上小节假设的神经网络为例，前向传播的公式如下：

1. $Z^{(2)} = W^{(1)} \cdot X + B^{(1)}$
2. $A^{(2)} = Sigmoid(Z^{(2)})$
3. $Z^{(3)} = W^{(2)} \cdot A^{(2)} + B^{(2)}$
4. $A^{(3)} = Softmax(Z^{(3)})$
5. $L = - \sum_{i=1}^n y_i logA^{(3)}$

当我们求$\frac{\partial L}{\partial B^2}$时:

$$\frac{\partial L}{\partial B^2} = \frac{\partial L}{\partial A^3} \cdot \frac{\partial A^3}{\partial Z^3} \cdot \frac{\partial Z^3}{\partial B^2}$$

由于$\frac{\partial Z^3}{\partial B^2}$的结果为1，所以：

$$\frac{\partial L}{\partial B^2} = \frac{\partial L}{\partial A^3} \cdot \frac{\partial A^3}{\partial Z^3} \cdot 1 = \frac{\partial L}{\partial Z^3}$$

当我们求$\frac{\partial L}{\partial B^1}$时:

$$\frac{\partial L}{\partial B^1} = \frac{\partial L}{\partial A^2} \cdot \frac{\partial A^2}{\partial Z^2} \cdot \frac{\partial Z^2}{\partial B^1} = \frac{\partial L}{\partial A^2} \cdot \frac{\partial A^2}{\partial Z^2} \cdot 1 = \frac{\partial L}{\partial Z^2}$$

由于偏置不依赖于输入，所以损失对偏置的导数等于损失对激活值的导数。

## 使用矩阵

上一小节中我们为了让大家清晰的理解前向传播与反向传播的过程，在计算时我们将所求的每个变量都单独列出。但是在实际场景中很少这样做，一般都会利用矩阵的性质来进行计算。

那么接下来我们将使用矩阵来优化计算。我们还是以昨天的例子来讲解。

#### 前向传播

1.隐藏层的输入：

$$Z^2 = W^1 \cdot X + B^1 = 
\begin{pmatrix}
0.1 & 0.2 \\ 
0.3 & 0.4 
\end{pmatrix}\\
\cdot 
\begin{pmatrix}
0.5\\
0.6
\end{pmatrix}
+
\begin{pmatrix}
0.1\\
0.2
\end{pmatrix}
= 
\begin{pmatrix}
0.27\\
0.59
\end{pmatrix}
$$
代码实现:
```python
Z1 = np.dot(W1, X) + b1
```

2.隐藏层使用激活函数Sigmoid：

$$A^2 = sigmoid(Z^1) =
\begin{pmatrix}
0.567\\
0.644
\end{pmatrix}
$$

代码实现:
```python

def sigmoid(x):
  return 1 / (1 + np.exp(-x))
  
A2 = sigmoid(Z1)
```
3.输出层的输入：

$$Z^3 = W^2 \cdot A^2 + B^2 = 
\begin{pmatrix}
0.7 & 0.8 \\ 
0.9 & 1.0 
\end{pmatrix}\\
\cdot
\begin{pmatrix}
0.567\\
0.644
\end{pmatrix}
+
\begin{pmatrix}
0.1\\
0.2
\end{pmatrix}
=
\begin{pmatrix}
1.012\\
1.354
\end{pmatrix}
$$
代码实现：
```python
Z3 = np.dot(W2, A2) + b2
```

4.输出层使用激活函数Softmax:

$$A^3 = softmax(Z^3) = 
\begin{pmatrix}
0.415\\
0.585
\end{pmatrix}
$$

代码实现:

```python
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # 为了数值稳定性
    return exp_x / np.sum(exp_x)
    
A3 = softmax(Z3)
```

5.使用交叉熵损失函数：

$$L = - \sum_{i=1}y_i logA^3  \approx 0.88$$

代码实现：

```python
def cross_entropy_loss(y_true, y_pred):
    return -np.sum(y_true * np.log(y_pred))
    
loss = cross_entropy_loss(y_true, A^3)
```

#### 反向传播

由于Softmax和交叉熵函数组合而成的复合函数求导，其结果等于输出值减去目标值，所以：

$$\frac{\partial L}{\partial Z^3} = A^3 - T = 
\begin{pmatrix}
0.415\\
0.585
\end{pmatrix}
-
\begin{pmatrix}
1\\
0
\end{pmatrix}
=
\begin{pmatrix}
-0.585\\
0.585
\end{pmatrix}
$$
接着求隐藏层到输出层权重的梯度：

$$\frac{\partial L}{\partial W^2} = \frac{\partial L}{\partial Z^3} \cdot A^2.(T)=
\begin{pmatrix}
-0.585\\
0.585
\end{pmatrix}
\cdot 
\begin{pmatrix}
0.567 & 0.585
\end{pmatrix}
=
\begin{pmatrix}
-0.332 & -0.377\\
0.332 & 0.377
\end{pmatrix}
$$

这里$A^2.(T)$指的是$A^2$的转置矩阵。

前面咱们说了，由于偏置不依赖于输入，所以损失对偏置的导数等于损失对激活值的导数。

$$\frac{\partial L}{\partial B^2} = \frac{\partial L}{\partial A^3} \cdot \frac{\partial A^3}{\partial Z^3} \cdot 1 = \frac{\partial L}{\partial Z^3} = \begin{pmatrix}
-0.585\\
0.585
\end{pmatrix}$$

接着更新权重和偏置：

$$W^2(new) = W^2(old) - \eta \cdot \frac{\partial L}{\partial W^2} = 
\begin{pmatrix}
0.7 & 0.8 \\ 
0.9 & 1.0 
\end{pmatrix}
-
0.01
\odot
\begin{pmatrix}
-0.332 & -0.377\\
0.332 & 0.377
\end{pmatrix}
=
\begin{pmatrix}
0.703 & 0.804\\
0.897 & 0.996
\end{pmatrix}
$$
$$B^2(new) = B^2(old) - \eta \cdot  \frac{\partial L}{\partial Z^3} = 
\begin{pmatrix}
0.1\\
0.2
\end{pmatrix}
-
0.01
\odot
\begin{pmatrix}
-0.585\\
0.585
\end{pmatrix}
=
\begin{pmatrix}
0.10585\\
0.19415
\end{pmatrix}
$$

代码实现：
```python
    # Softmax和交叉熵函数求导
    dZ3 = A3 - y_true
    # 隐藏层到输出层权重的梯度
    dW2 = np.dot(dZ3, A2.T)  
    # 计算偏置
    db2 = np.sum(dZ3, axis=1, keepdims=True) 
    
    #更新权重
    W2 -= learning_rate * dW2
    #更新偏置
    b2 -= learning_rate * db2
```

这里我们可以看见在计算偏置的时候使用sum函数，但是我们在推演的时候并没有进行求和。这主要是为了处理多批量样本训练时的情况。虽然在单个样本情况下db2和dZ3的值是相同的，但对于多个样本的情况，dZ3是一个矩阵，每一列对应一个样本。在这种情况下，需要对每个样本的偏置梯度进行求和，再根据学习率更新偏置。这里不展开，之后在讲解mini-batch的时候再详细说明。

在代码中我们对某个变量求偏导，在命名时会在前面加上d。dZ3就是$\frac{\partial L}{\partial Z^3}$。

接着继续更新输入层到隐藏层的权重和偏置。

$$\frac{\partial L}{\partial A^2} = \frac{\partial L}{\partial Z^3}(T) \cdot W^2 =
\begin{pmatrix}
-0.585 & 0.585\\
\end{pmatrix}
\cdot
\begin{pmatrix}
0.7 & 0.8 \\ 
0.9 & 1.0 
\end{pmatrix}
=
\begin{pmatrix}
0.116 & 0.117\\
\end{pmatrix}
$$
由于隐藏层使用的Sigmoid激活函数，所以：

$$\frac{\partial A^2}{\partial Z^2} = A^2 \odot (1 - A^2) =
\begin{pmatrix}
0.567\\
0.644
\end{pmatrix}
\odot
(1 \\
-
\begin{pmatrix}
0.567\\
0.644
\end{pmatrix}
)
=
\begin{pmatrix}
0.246\\
0.229
\end{pmatrix}
$$

这里注意$\odot$符号，这里指的是**Hadamard乘积**，对于相同形状的矩阵A，B,将相同位置的元素相乘，由此产生的矩阵称为矩阵A，B的Hadamard乘积,用A$\odot$B表示。

上面我求的是$\frac{\partial A^2}{\partial Z^2}$，那么$\frac{\partial L}{\partial Z^2}$就等于：

$$\frac{\partial L}{\partial Z^2} = \frac{\partial L}{\partial A^2} \cdot \frac{\partial A^2}{\partial Z^2} =
\begin{pmatrix}
0.116 \\
0.117
\end{pmatrix}
\odot
\begin{pmatrix}
0.246 \\
0.229
\end{pmatrix}
=
\begin{pmatrix}
0.029 \\
0.027
\end{pmatrix}
$$
这里由于$\frac{\partial A^2}{\partial Z^2}$的形状是2*1的矩阵，没办法直接做乘法，所以这里是进行了转置。

接着就能求出权重的梯度：

$$\frac{\partial L}{\partial W^1} = \frac{\partial L}{\partial Z^2} \cdot X.(T) =
\begin{pmatrix}
0.029 \\
0.027
\end{pmatrix}
\cdot
\begin{pmatrix}
0.5 & 0.6
\end{pmatrix}
=
\begin{pmatrix}
0.014 & 0.017 \\ 
0.013 & 0.016 
\end{pmatrix}
$$

而偏置的梯度就等于：

$$\frac{\partial L}{\partial B^1} = \frac{\partial L}{\partial Z^2} = 
\begin{pmatrix}
0.029 \\
0.027
\end{pmatrix}$$

最后更新权重和梯度：
$$W^1(new) = W^1(old) - \eta \cdot \frac{\partial L}{\partial W^1} = 
\begin{pmatrix}
0.1 & 0.2 \\ 
0.3 & 0.4 
\end{pmatrix}
-
0.01
\odot
\begin{pmatrix}
0.014 & 0.017 \\ 
0.013 & 0.016 
\end{pmatrix}
=
\begin{pmatrix}
0.09986 & 0.19983\\
0.29987 & 0.39984
\end{pmatrix}
$$
$$B^1(new) = B^1(old) - \eta \cdot  \frac{\partial L}{\partial Z^2} = 
\begin{pmatrix}
0.1\\
0.2
\end{pmatrix}
-
0.01
\odot
\begin{pmatrix}
0.029 \\
0.027
\end{pmatrix}
=
\begin{pmatrix}
0.09971\\
0.19973
\end{pmatrix}
$$

代码实现：
```python

def sigmoid_derivative(x):
    return x * (1 - x)
#计算A2的导数
dA2 = np.dot( dZ3.T,W2)
#计算Z2的导数
dZ2 = dA2.T * sigmoid_derivative(A2)
#计算W1的导数
dW1 = np.dot(dZ2, X.T) 
#计算偏置的导数
db1 = np.sum(dZ2, axis=1, keepdims=True) 
#更新权重  
W1 -= learning_rate * dW1
#更新偏置
b1 -= learning_rate * db1
```

### 完整代码实现

```python
import numpy as np

# 激活函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def softmax(x):
    exp_x = np.exp(x - np.max(x))  # 为了数值稳定性
    return exp_x / np.sum(exp_x)

# 前向传播
def forward(X, W1, b1, W2, b2):
    # 输入层到隐藏层
    Z2 = np.dot(W1, X) + b1
    A2 = sigmoid(Z2)


    # 隐藏层到输出层
    Z3 = np.dot(W2, A2) + b2
    A3 = softmax(Z3)
    

    return A3, A2

# 交叉熵损失函数
def cross_entropy_loss(y_true, y_pred):
    return -np.sum(y_true * np.log(y_pred))

# 反向传播
def backward(X, A3, A2, y_true, W1,W2,b1,b2, learning_rate):
    # 输出层梯度
    dZ3 = A3 - y_true
    
    dW2 = np.dot(dZ3, A2.T)  # 输出层的权重
    
    db2 = np.sum(dZ3, axis=1, keepdims=True)  # 输出层的偏置
   
    
    # 计算隐藏层梯度
    dA2 = np.dot(dZ3.T,W2)
    
    dZ2 = dA2.T * sigmoid_derivative(A2) # 使用sigmoid的导数
    
    dW1 = np.dot(dZ2, X.T)  # 输入层到隐藏层的权重
   
    db1 = np.sum(dZ2, axis=1, keepdims=True)  #隐藏层的偏置
  

    # 更新权重和偏置
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2

# 初始化权重、偏置、输入和标签
W1 = np.array([[0.1, 0.2], [0.3, 0.4]])  # 输入层到隐藏层的权重
b1 = np.array([[0.1, 0.2]]).T  # 输入层到隐藏层的偏置
W2 = np.array([[0.7, 0.8], [0.9, 1.0]])  # 隐藏层到输出层的权重
b2 = np.array([[0.1, 0.2]]).T  # 隐藏层到输出层的偏置

# 输入 (2x1矩阵)
X = np.array([[0.5, 0.6]]).T  # 2行1列，表示两个输入节点

# 真实标签 (2x1矩阵)
y_true = np.array([[1, 0]]).T  # 真实输出

# 设置学习率
learning_rate = 0.01

# 训练一轮
A3, A2 = forward(X, W1, b1, W2, b2)
loss = cross_entropy_loss(y_true, A3)
print(f"Loss: {loss}")

# 反向传播更新参数
backward(X, A3, A2, y_true,W1, W2,b1,b2, learning_rate)

print(f"Updated weights W1: \n{W1}")
print(f"Updated biases b1: \n{b1}")
print(f"Updated weights W2: \n{W2}")
print(f"Updated biases b2: \n{b2}")

```

结合完整的代码，可以对整个神经网络前向传播和反向传播有更直观的认识，对神经网络的理解不仅仅停留在理论阶段。

## 总结
本小节讲解了如何更新偏置，以及使用矩阵的方式来优化计算，最后给出了完整的代码实现。对于反向传播时使用了很多矩阵转置，大家可以结合前一小节进行对比，然后亲手敲一下代码，运行调试一下。