## 前文提要
在上一小节中，我们介绍了输出层使用的激活函数Softmax。Softmax可以将每个节点的值转化为概率进行输出。这一小节我们将介绍如何判定输出结果与我们预期值的差距。

## 损失函数
当我们在输出层使用完Softmax函数得到每个节点的概率以后，我们就完成的整个神经网络的前馈，也就是从输入层到输出层。拿到输出结果以后，我们要判断这个结果是否是我们想要的。于是我们将输出结果与我们的预期值进行对比。

通常在训练神经网络时，我们会提供数据集，在每组训练时会从数据集中拿出输入数据和预期输出值。这种训练称为监督学习。

我们将输出结果与我们的预期值进行比较后发现不是我们想要的，那么我们就需要知道输出结果与预期值之间相差多少，相差的值就是误差。我们需要用这个误差值来指导我们重新调整神经网络。

要知道误差是多少，最简单的办法就是用预期值减去输出结果。但是这种方法在神经网络中并不常用。

原因在于简单减法是一种线性关系，这种线性关系无法灵敏的反映细微的变化，在使用时效率很低。

所以在实际使用过程中，我们通常使用损失函数来衡量输出结果与真实值之间的差距。损失函数的输出是一个标量，通过最小化损失函数，可以指导优化神经网络参数，使得预测结果更加准确。

## 均方误差（Mean Squared Error,MSE）

均方误差一般用于回归问题，比如预测体重，股票预测等。


公式：

$MSE = \frac{1}{n}\sum_{i=0}^n (y_i - \widehat{y_i})^2$

- $y_i$是第i个节点的真实值。

- $\widehat{y_i}$是第i个节点的神经网络输出值。

- n是输出层节点数量。

上面的数据公式用通俗的语言表达就是，计算输出层每个节点真实值与输出值之间的平方差，然后取平均值。

举例：假设有以下真实值和输出值：

- 真实值 y:[0.2,0.5,0.8]
- 输出值 $\widehat{y}$:[0.1,0.6,0.75]

首先我们可以逐个节点计算平方误差。

1.对于第一个输出节点(i=1):

$(y_1-\widehat{y_1})^2 = (0.2 - 0.1)^2 = 0.01$

2.对于第二个输出节点(i=2):

$(y_2-\widehat{y_2})^2 = (0.5 - 0.6)^2 = 0.01$

2.对于第三个输出节点(i=3):

$(y_3-\widehat{y_3})^2 = (0.8 - 0.75)^2 = 0.0025$


然后，对这些误差平方取平均：

$MSE = \frac{1}{3} \times (0.01 + 0.01 + 0.0025) = \frac{0.0225}{3} \approx 0.0075$

这样我们就计算出了输出层为3个节点的均方误差值。

注意：这里假设的是训练时只有一组数据。如果一次训练中包含了多次数据则以上公式会有略微的改变，这种情况我们将在介绍Batch Training时介绍。

代码实现：

```python
# 假设真实值和预测值如下
y_true = [0.2, 0.5, 0.8]    # 真实值
y_pred = [0.1, 0.6, 0.75]   # 预测值

import numpy as np
def mean_squared_error(y,t,n):
    return np.sum((y-t)**2) / n
    
#另一种实现
def mean_sequared_error(y_true,y_pred):
  return np.mean((y_true - y_pred) ** 2)

```

代码中使用了两个numpy库的函数：

1.sum 函数：函数用于对数组中的元素进行求和。

2.mean函数：函数用于计算数组中元素的算术平均值。相当与使用了sum函数后再除以总数量。


## 交叉熵(Cross-Entropy Loss)

在处理分类问题的时候，我们一般使用交叉熵，例如图像中的数字是0-10中的哪一位。它衡量的是预测分布与真实分布之间的差异。

公式：

$Loss = - \sum_{i=1}y_i\log\widehat{y_i}$

- $y_i$表示目标。

- $\widehat{y_i}$表示神经网络输出值。

- log以e为底数的自然对数。

这里我们注意到公式前面有一个负号，为什么需要这个负号呢？

因为$\widehat{y_i}$输出的是概率值，这个值时在0-1之间。所以$\log\widehat{y_i}$的取值范围在负无穷到0。$\widehat{y_i}$的值越接近1，$\log\widehat{y_i}$的值越接近与0。$\widehat{y_i}$的值越接近0,$\log\widehat{y_i}$的值越接近与负无穷。在交叉熵的公式前面加一个负号，使得整体结果为非负数。因为我们的目的是追求损失函数的最小化，而损失应该是一个非负值。

我们看以上公式可能觉得很复杂，又是求和又是对数函数。但是实际上，训练数据给出的目标值为独热编码（One-hot Encoding）时，也就是正解的时候为1，其余的都为0时。以上公式就只需要计算正解时的对数。

例如：

正解:[0,1,0,0,0,0,0,0,0,0]

输出结果为:[0.3,0.6,0.1,0.2,0.12,0.3,0.11,0.145,0.23,0.22]

这时候值需要计算第二位，也就是$- 1 \times \log0.6 = 0.5108256237659907$

你可能会问了，既然只需要计算正解对应的对数，为什么公式里面还需要一个求和符号？

这是为了保证交叉熵公式的通用性，因为有时候给出的目标值并不是独热编码，而是软标签。

例如：我们有一个三分类问题，正解给出的是软标签，每个类别的概率分布如下：

正解：[0.7,0.2,0.1]

输出结果为:[0.6,0.3,0.1]

此时，交叉熵损失的计算过程如下：
$Loss = - (0.7 \times \log0.6 + 0.2 \times log0.3 + 0.1 \times log0.1) \approx 0.7136$

理解了公式以后，我们来看看代码实现：

```python
import numpy as np 

def cross_entropy(y_true,y_pred):
  # 使用np.log计算对数时，避免出现log(0)的情况
  y_pred = np.clip(y_pred,1e-7,1 - 1e-7)
  
  loss = - np.sum(y_true * np.log(y_pred))
  
  return loss
  
# 示例数据
y_true = np.array([0.7, 0.2, 0.1])  # 真实标签（概率分布）
y_pred = np.array([0.6, 0.3, 0.1])  # 模型预测概率

# 计算交叉熵损失
loss = cross_entropy(y_true, y_pred)

print(f"交叉熵损失: {loss}")
```

代码中使用了numpy库的两个函数：

1.clip函数：这个函数用于确保输出的概率y_pred不会为0或1，这样可以避免计算时出现数学错误。（如log0 会导致无穷大）,1e-7是一个非常小的数，用来保证y_pred不为0或1。

2.log函数：用来计算y_pred的对数。

以上代码中将y_true的值改为独热编码即可测试在独热编码时的交叉熵。


## 总结
本小节讲解了用于衡量输出结果与目标结果差距的常用损失函数。其中均方误差常用于回归问题，而交叉熵常用于分类问题。