## 前文提要
前面几篇文章我们介绍了计算机和人类擅长的事情，以及神经元的两大特性：

1.模糊性

2.忽略微小的输入信号

我们用感知机去模拟神经元的特性，并用感知机去实现了逻辑电路。这一切都是为了引出今天要介绍的内容---神经网络。

## 神经网络

神经网络顾名思义就是由多个神经元组成的网络，跟上节说介绍的多层感知机一样，神经网络分为：

1.输入层（Input Layer）：
接收输入数据，不做任何其他的事情。
每个输入节点对应一个特征，例如图像的像素值或文本的词向量。

2.隐藏层（Hidden Layers）：介于输入层和输出层之间，可以有多个。每个隐藏层节点通过加权的连接接收来自上一层节点的输入。隐藏层节点通常通过激活函数进行非线性变换，以提高模型的表达能力。

3.输出层（Output Layer）：生成最终的输出结果。输出节点的数量和类型取决于具体的任务，例如分类任务中的类别数量。

![](https://files.mdnice.com/user/70350/209ee27d-3058-4c05-adb6-0dfb674d98f9.png)


上图是一个简单的3层神经网络（有些教材输入层不算，看作是2层神经网络）。数据从输入层传入，输入层不做任何变换，直接将数据进行输出。而隐藏层的输入数据则是输入层各节点加权求和的结果。还记得我们之前说的，信号也是有权重的。例如对于隐藏的第一个节点，他的输入数据来自前一层两个节点的输出信号，并且这两个信号都有各自的权重${w_1}_1$,${w_2}_1$。所以对于隐藏层第一个节点来说，它的输入数据为：

$z_2^1 = x_1*{w_1}_1 + x_2*{w_2}_1$

这里的$z_2^1$表示第二层第一个节点的输入数据。其他节点的输入数据都等于前一层的输出数据和权重的加权求和。

## 偏置

在这里我们要考虑一种特殊的情况，也就是当输入数据都为0时。例如输入层的两个节点$x_1$=0,$x_2$=0。那么对于隐藏层的所有节点来说，输入的结果都为0。

$z_2^1 = 0*{w_1}_1 +0*{w_2}_1$ = 0

如果激活函数使用的是ReLU函数，那么就会导致节点不会被激活（输出为0）。虽然在多次训练后，可以将参数调整到适应各种输入情况，但是可能需要更多的训练时间和更复杂的权重调整过程。（这句话大家目前可能不理解，先有个印象，学习完神经网络的整个流程再回头看，就能明白。）

怎么解决这个问题？很简单，就是在计算出结果后再加上一个值。而这个值就叫偏置。

在没有偏置的时候，如果输入信号$x_1$=0,$x_2$=0，那么无论权重怎么变，输出值都会固定在一个特定值。这样会导致：

1.模型表达能力不足：
神经网络无法学习复杂的非线性关系，因为激活函数的输出缺乏多样性。

2.梯度消失或爆炸：
在深层神经网络中，如果某些层的激活函数输出始终固定，反向传播时这些层的梯度可能会消失或变得非常大，影响模型的训练效果。

在使用偏置后：

$z_2^1 = 0*{w_1}_1 +0*{w_2}_1 + b$ 

即使$x_1$=0,$x_2$=0，$z_2^1$的值也等于$b$。通过调整偏置 $b$，我们可以使得激活函数的输入$z$在不同范围内变化。

可见增加一个小小的偏置能有这么大的作用。

## Sigmoid函数

我们在之前的文章中介绍了激活函数，当时使用的激活函数是阶越函数，当输入数据超过某个阈值时输出为1，小于某个阈值时输出为0。

阶越函数有什么问题？

1.可微性

不可微，只有在跳跃点上才会发生变化，导数不存在或不稳定，无法用于梯度下降优化

2.平滑输出

输出只有0和1两个值，这使得网络无法表达细微的变化。

3.梯度信息

导数几乎在所有地方都是0，导致在反向传播时梯度为0，权重无法更新，这种现象称为“梯度消失”。（后面我们会学习梯度下降法，到时候会详细讲解）

4.学习能力

由于其输出有限且不连续（只能输出0和1，且图像上有断层），难以捕捉复杂的模式，限制了网络的学习能力。

所以最早的感知机会使用阶越函数，但是后来的神经网络基本上都不会再用它。这里我们给大家介绍一个最常用的激活函数---**Sigmoid函数**。（其他的激活函数我们会在神经网络的优化部分给大家分享）。

Sigmoid函数的公式：

$\sigma(x) = \frac{1}{1+e^{-x}} $

其中e是纳皮尔常数2.7182...。

图像如下：
![](https://files.mdnice.com/user/70350/86ee6f2e-d71a-47cb-8dcc-6f174ee0639f.png)

Sigmoid函数有如下特点：

1.输出范围

Sigmoid函数的输出范围是0到1。无论输入数据是什么，sigmoid函数的输出都会被限制在这个范围内。这种性质使得它在二分类问题中很有用，因为输出可以被解释为概率值。

2.平滑性和可微性

Sigmoid函数是平滑且连续的，这意味着它在所有点上都是可微的。
可微性对于使用梯度下降法进行神经网络的训练非常重要，因为它允许计算梯度并更新网络的权重。

3.单调递增

Sigmoid函数是单调递增的函数，随着输入的增加，输出$\sigma(x)$也会增加。


4.中心对称

Sigmoid函数在点 (0,0.5)对称。即，当输入x为0时，输出为0.5。

5.渐近性

当输入趋近于正无穷大时，$\sigma(x)$趋近于1。

当 输入 趋近于负无穷大时，$\sigma(x)$ 趋近于0。

这种渐近性质意味着在极端输入值下，输出变化趋于平缓。

从Sigmoid函数的特性我们能看到，它很好的解决了阶越函数存在的问题。

代码实现:
```python
  import numpy as np
  
  def sigmoid(x):
    return 1/(1+np.exp(-x))
```

这里使用了numpy库中的exp函数，该函数对应$e^{-x}$。

## 非线性函数

不知道大家发现没有，不论是之前的阶越函数，还是Sigmoid函数，它们都属于非线性函数。

那什么叫线性函数？什么叫非线性函数呢？

线性和非线性是数学和计算机科学中的两个重要概念，尤其在机器学习和神经网络中经常被提及。

**线性**

线性指的是一种关系或函数，其输出是输入的线性组合，换句话说，变量之间的关系可以用一条直线（在二维空间中）或一个超平面（在高维空间中）来表示。线性关系具有以下特征：

1.加性：如果$f(x)$是一个线性函数，那么对于任意的来个那个输入$x_1$和$x_2$，有$f(x_1+x_2) = f(x_1) + f(x_2)$。

2.齐次性：如果$f(x)$是一个线性函数，那么对于任意的输入$x$和常数$a$,有$f(a*x) = a*f(x)$。

例如：
$y = x_1w_1+x_2w_2+b$

**非线性**

非线性指的是一种关系或函数，其输出不是输入的线性组合。非线性关系可以表现为曲线或更复杂的形状，不能用一条直线或一个超平面来表示。非线性关系具有以下特征：

1.非加性：不满足加性原则，$f(x_1+x_2) \neq f(x_1) + f(x_2)$。
2.非齐次性：不满足齐次性原则，$f(a*x) \neq a*f(x)$。

例如：

$y = sin(x)$

那么为什么激活函数都要使用非线性函数呢？

因为使用线性函数的话，加深神经网络的层数就没有意义了。

线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。

例如把函数$\sigma(x)= cx$ 作为激活函数，把$y(x)=\sigma(\sigma(\sigma(x)))$的运算对应3层神经网络，这个运算会进行$y(x)=c*c*c*x$的乘法运算。但是同样的处理可以由$y(x)=ax，a=c^3 $这一乘法运算来表示。所以使用多层网络带来的优势就荡然无存。也就跟之前单层感知机一样了。

还记得我们为什么要使用多层感知机吗？因为单层感知机只能处理线性可分问题，比如通过一条直线（在二维空间中）或一个平面（在三维空间中）将数据集的不同类别分开。

而多层神经网络在隐藏层中使用了非线性的激活函数（Sigmoid,ReLU等），使得隐藏层的输出是输入的非线性变换。通过堆叠多层，神经网络能够捕捉到数据中的复杂非线性关系。

## 总结

本小节讲述了最简单的神经网络，神经网络由3大块构成，输入层，隐藏层，输出层。并且讲解了将输入数据和权重进行加权求和得到隐藏层的输入数据，再把输入数据给激活函数得到输出数据。

文中可能有些知识点大家暂时不能理解，不过没关系，随着接下来的学习，大家会豁然开朗。由于文章长度都原因，我们把知识点拆开了，后面会用一个简单的例子给大家完整的串一遍。