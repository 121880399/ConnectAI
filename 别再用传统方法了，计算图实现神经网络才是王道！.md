## 前文提要

前一小节讲解了计算图以及计算图中各节点的前向传播与反向传播，并给出了代码实现。这一小节我们用计算图来实现一个简单的神经网络。

## 计算图实现神经网络

假设有一个简单的三层神经网络。隐藏层的激活函数使用Sigmoid函数，输出层的激活函数使用Softmax函数，损失函数使用交叉熵,标签使用One-hot编码。使用计算图的方式来实现这个神经网络。


代码实现：
```python
import numpy as np
from collections import OrderedDict


class Affine:
  def __init__(self,W,b):
    self.W = W
    self.b = b
    self.x = None
    self.dW = None
    self.db = None
    
  def forward(self,x):
    self.x = x
    out = np.dot(self.W,x)+self.b
    
    return out
    
  def backward(self,dout):
    print (f'dout:{dout}')
    print (f'w:{self.W}')
    dx = np.dot(self.W.T,dout)
    self.dW = np.dot(dout,self.x.T)
    self.db = np.sum(dout,axis = 1,keepdims=True)
    print (f'db:{self.db}')
    return dx

class sigmoid:
  def __init__(self):
    self.out = None
  
  def forward(self,x):
    self.out = 1/(1+np.exp(-x))
    return self.out
    
  def backward(self,dout):
    dx = dout * (1.0 - self.out) * self.out
    return dx

  
class SoftmaxWithLoss:
    def __init__(self):
      #损失
      self.loss = None
      #输出
      self.sigma = None
      #标签
      self.t = None
    
    def cross_entropy_error(self,y_true,y_pred):
        # 使用np.log计算对数时，避免出现log(0)的情况
        y_pred = np.clip(y_pred,1e-7,1 - 1e-7)

        loss = - np.sum(y_true * np.log(y_pred))

        return loss
  
    def softmax(self,z):
        print(f'z:{z}')
        exp_z = np.exp(z)
        print(f'exp_z:{exp_z}')
        sum = np.sum(exp_z)
        out  = exp_z / sum
        
        return out
    
    def forward(self,x,t):
      self.t = t
      print(f'x:{x}')
      self.sigma = self.softmax(x)
      self.loss = self.cross_entropy_error(self.sigma,self.t)
      
      return self.loss
    
    def backward(self,dout = 1):
      dx = self.sigma - self.t
      
      return dx
    
class ThreeLayerNet:
  
  def __init__(self,input_layer_size,hidden_layer_size,output_layer_size):
    #初始化权重和偏置
    self.params = {}
    self.params['W1'] = np.random.randn(hidden_layer_size,input_layer_size)
    self.params['b1'] = np.random.randn(hidden_layer_size,1)
    self.params['W2'] = np.random.randn(output_layer_size,hidden_layer_size)
    self.params['b2'] = np.random.randn(output_layer_size,1)

    #生成层
    self.layers = OrderedDict()
    self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1'])
    self.layers['Sigmoid1'] = sigmoid()
    self.layers['Affine2'] = Affine(self.params['W2'],self.params['b2'])
    self.lastLayer = SoftmaxWithLoss()

  def predict(self,x):
    for layer in self.layers.values():
      x = layer.forward(x)

    return x
  
  def loss(self,x,t):
    y = self.predict(x)
    return self.lastLayer.forward(y,t)
  
  def gradient(self,x,t):
    #正向传播
    self.loss(x,t)

    #反向传播
    dout = 1
    dout = self.lastLayer.backward(dout)

    layers = list(self.layers.values())
    layers.reverse()
    for layer in layers:
      dout = layer.backward(dout)
    
    # 保存梯度
    grads = {}
    grads['W1'] = self.layers['Affine1'].dW
    grads['b1'] = self.layers['Affine1'].db
    grads['W2'] = self.layers['Affine2'].dW
    grads['b2'] = self.layers['Affine2'].db

    return grads
   




# 输入 (2x1矩阵)
X = np.array([[0.1], [0.2]])  # 2行1列，表示两个输入节点

# 真实标签 (2x1矩阵)
y_true = np.array([[1], [0]])  # 真实输出

# 设置学习率
learning_rate = 0.01

## 创建神经网络
network = ThreeLayerNet(2, 2, 2)

print(f'pre:{network.params}')

grads = network.gradient(X,y_true)

#更新
for key in network.params:
  print(f'key:{key}')
  print(f'grads[key]:{grads[key]}')
  network.params[key] -= learning_rate * grads[key]

print(f'after:{network.params}')

```

**第一步**

我们将Affine层，Sigmoid层，SoftmaxWithLoss层都定义成类。

**第二步**

创建一个只有3层的神经网络(ThreeLayerNet)，这个类的构造函数允许我们指定输入层、隐藏层、输出层分别有多少个节点。

在构造函数中，我们首先使用randn函数来初始化每一层的权重和偏置。randn函数用于生成一个指定形状的数组，数组中的元素是从标准正态分布中随机选取的值。例如在初始化权重“W1”时，我们将隐藏层的节点数设置为行，将输入层节点数设置为列，那么将生成一个二维数组。为什么将隐藏层的节点数设置为行呢？这跟我们之前规定的权重命名有关系，我们之前规定权重下标第一个代表的是目标节点的序号，第二个代表是出发节点的序号。所以$w_{21}$代表的下一层的第二个节点与本层第一个节点的权重。

接着使用了一个叫OrderedDict的数据结构。该数据结构能够保持数据插入的顺序。
我们将计算图中介绍的各层创建出来，放入OrderedDict这个有序字典。

前向传播和反向传播时就遍历OrderedDict，调用其forward方法和backward方法。

**第三步**

创建输入数据和标签，以及学习率。调用创建好的神经网络并且更新其参数。

整个代码非常的清晰，虽然整体代码量增加了，但是可复用性提高了。这就是计算图的魅力，我们可以将每一步抽象成单独的类，类中实现这一步相应的前向传播和反向传播的逻辑。

## 总结

本小节讲解了如何使用计算图的方式，来实现一个简单神经网络实例，让我们对计算图有了一个整体的认识。