## 前文提要
在上一小节中，我们使用矩阵来优化神经网络的前向传播和反向传播计算，并且给出了代码实现。我们在讲述反向传播时是通过数学式的方式进行讲解，这样做的好处在于严密简洁，不足之处在于枯燥不易理解。所以在这一小节中，我们讲通过计算图来加深大家对反向传播的理解。

## 计算图

先看一个简单的场景，小明要去超市买10斤的大米和10斤面粉，大米每斤2.6元，面粉每斤2.4元，一共花费50元。我们将这个场景以计算图的形式呈现出来。

![](https://files.mdnice.com/user/70350/9460a29a-5c6b-4d95-903c-b0e977da0735.png)

可以看到每个圆圈都是一个操作符，操作符的左边是输入数据，右边是输出数据。非常的清晰直观。我们将从左到右的计算可以看作是前向传播。从右到左的计算看作反向传播。

现在我们想知道面粉价格的上涨会多大程度上影响最终支付的金额？也就是最终支付价格关于面粉价格的导数。我们设面粉价格为x，最终支付金额为L，
那么最终支付金额$L = 2.6*10+x*10$。$\frac{\partial L}{\partial x} = 10$。

![](https://files.mdnice.com/user/70350/0a490599-f346-420f-a3da-29cba23f431b.png)

也就是说当面粉的价格上涨1元，最终支付价格会增加10元。

你可能会说，这里之所以能求出面粉价格对最终价格的影响，是运用了求导法则，这跟前面讲的数学式求反向传播一样呀？

那么接下来我们就讲一些不一样的。

### 加法节点的反向传播

为了更好的说明计算图的特点，我们将任务进行拆分。首先我们分析z=x+y这函数，这个函数分别对x和y求偏导等于:
$$\frac{\partial z}{\partial x} = 1$$
$$\frac{\partial z}{\partial y} = 1$$

用计算图表示出来，如下图所示。

![](https://files.mdnice.com/user/70350/5c8a3cec-217e-423b-8b05-b0d272f77f78.png)

我们假设$\frac{\partial L}{\partial z}$是反向传播过来的导数值，当到达“+”号这个节点时，由于$\frac{\partial z}{\partial x}，
\frac{\partial z}{\partial x} $都等于1。所以输入的值$\frac{\partial L}{\partial z}$会原封不动的流向下一个节点。也就是说如果上游传过来的$\frac{\partial L}{\partial z}$为13，那么经过“+”号节点后，还是为13。加法节点的反向传播只是将输入信号再输出到下一个节点。

代码实现:
```python
class AddLayer:

  def __init__(self):
    pass
    
  def forward(self,x,y):
    output = x+y
    return output
    
  def backward(self,dout):
    dx = dout * 1
    dy = dout * 1
    return dx,dy
```

### 乘法节点的反向传播

接下来我们考虑乘法节点的反向传播，我们假设有函数z=xy。则分别对x,y求偏导：

$$\frac{\partial z}{\partial x} = y$$

$$\frac{\partial z}{\partial y} = x$$

用计算图表示，如下：

![](https://files.mdnice.com/user/70350/7ce7f600-a270-4322-8986-64f8f3c8ab5d.png)

从图中我们可以看到，乘法节点的反向传播会将上游传递过来的值乘以前向传播时输入信号的“翻转值”然后传递给下游。前向传播时信号是x,那么在反向传播时则乘以y。正向传播时输入信号是y，反向传播时则乘以x。

代码实现:
```python
class MulLayer:
  #由于在反向传播时要用到输入的x,y值，所以需要保存下来
  def __init__(self):
    self.x = None
    self.y = None
  
  def forward(self,x,y):
    self.x = x
    self.y = y
    output = x *y
    
    return output
    
  def backward(self,dout):
    #翻转
    dx = dout * self.y
    dy = dout * self.x
    
    return dx,dy
    
```

### 结合例子

现在我们学习了加法节点和乘法节点的反向传播，我们再结合前面小明去超市买大米和面粉的例子，看看如何使用。

![](https://files.mdnice.com/user/70350/ff8e26ff-8a1f-46a7-a9b1-7f9b928c956c.png)

首先反向传播时起始值为1，经过加法节点后，由于加法节点值将上游值原封不动的传递到下游，所以值不会改变。接着经过乘法节点，我们以大米为例，当经过节点后要将上游的值乘以前向传播时输入信号的“翻转值”。比如求大米价格的导数，就要用上游值1乘以10。求大米重量的导数，就要用上游值乘以2.6。

大家发现没有，如果能将复杂的复合函数用计算图表示出来，那么在反向传播的时候就可以不用麻烦的用求导公式，而只需要用上游传递过来的值根据当前节点的规则进行计算。

代码实现：
```python
rice = 2.6
rice_num = 10

flour = 2.4
flour_num = 10

rice_layer = MulLayer()
flour_layer = MulLayer()
add_rice_flour_layer = AddLayer()

#前向传播
rice_price = rice_layer.forward(rice,rice_num)
flour_price = flour_layer.forward(flour,flour_num)
all_price = add_rice_flour_layer.forward(rice_price,flour_price)

print(all_price)

#反向传播
dprice = 1
drice_price,dflour_price = add_rice_flour_layer.backward(dprice)
drice,drice_num = rice_layer.backward(drice_price)
dflour,dflour_price = flour_layer.backward(dflour_price)

print(drice,drice_num,dflour,dflour_price)

```

代码实现很简单，首先声明需要的大米和面粉的单价和数量，然后声明需要的节点。有了这些之后就可以开始前向传播和反向传播。


### Affine层

前面我们讲解了简单的加法节点和乘法节点在计算图中的应用，那么计算图在神经网络中该如何应用呢？

在神经网络中，Affine（仿射）层是指一种线性变换层，主要功能是对输入进行线性变换，然后加上一个偏置项。数学形式可以表示为：
$$Z=WX+b$$

其中：
- W是权重矩阵。
- X是输入数据。
- b是偏置。
- Z是输出。

用代码表示就是:
```python
Z = np.dot(W,X)+B
```

用计算图表示如下：
![](https://files.mdnice.com/user/70350/0c6759a8-738b-4a5d-89c7-38ef80d841dc.png)



在反向传播时假设：

$$X=\begin{pmatrix}
x_1 \\
x_2 
\end{pmatrix}$$

$$W =\begin{pmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22}  
\end{pmatrix} $$

$$B = \begin{pmatrix}
b_{1} \\
b_{2}  
\end{pmatrix} $$

令$C=W \cdot X$,那么
$$c_1 = w_{11} \cdot x_1 + w_{12} \cdot x_2$$

$$c_2 = w_{21} \cdot x_1 + w_{22} \cdot x_2$$

对每个输入元素X求导：
$$\frac{\partial L}{\partial x_1} = \frac{\partial L}{\partial c_1} \cdot w_{11} + \frac{\partial L}{\partial c_2} \cdot w_{21}$$

$$\frac{\partial L}{\partial x_2} = \frac{\partial L}{\partial c_1} \cdot w_{12} + \frac{\partial L}{\partial c_2} \cdot w_{22}$$

从上面的推导可以看出来，对于每个输入元素$x_i$其梯度是W转置矩阵的每一行元素与$\frac{\partial L}{\partial C}$的每一列与的点积。也就是：
$$\frac{\partial L}{\partial X} = W^T \cdot \frac{\partial L}{\partial C} $$

对于每个权重W求导：
$$\frac{\partial L}{\partial w_{11}} = \frac{\partial L}{\partial c_1} \cdot x_1$$
$$\frac{\partial L}{\partial w_{12}} = \frac{\partial L}{\partial c_1} \cdot x_2$$
$$\frac{\partial L}{\partial w_{21}} = \frac{\partial L}{\partial c_2} \cdot x_1$$
$$\frac{\partial L}{\partial w_{22}} = \frac{\partial L}{\partial c_2} \cdot x_2$$

转换为矩阵乘法就是:
$$\frac{\partial L}{\partial W} = \frac{\partial L}{\partial C} \cdot X^T= 
\begin{pmatrix}
\frac{\partial L}{\partial c_1} \\
\frac{\partial L}{\partial c_2}\\
\end{pmatrix}
\cdot
\begin{pmatrix}
x_1 & x_2
\end{pmatrix}
$$

而对于每个偏置B求导：
$$\frac{\partial L}{\partial B} = \frac{\partial L}{\partial Z}$$

用计算图表示如下：
![](https://files.mdnice.com/user/70350/6f5cb9c8-b046-476d-a572-58abbc464580.png)

从图中我们可以很清晰的看懂Affine层的反向传播过程，通过加号节点时会原封不动的进行数据传递，通过dot（点乘）符号时，只需要将上游传递过来的值乘以前向传播时对面矩阵的转置。

代码实现：
```python
class Affine:
  def __init__(self,W,b):
    self.W = W
    self.b = b
    self.x = None
    self.dW = None
    self.db = None
    
  def forward(self,x):
    self.x = x
    out = np.dot(self.W,x)+b
    
    return out
    
  def backward(self,dout):
    dx = np.dot(self.W.T,dout)
    self.dW = np.dot(dout,self.x.T)
    self.db = np.sum(dout,axis = 1,keepdims=True)
    
    return dx
```

### 激活函数

输入数据与权重进行加权求和后，就要进行激活函数的运算，这里我们介绍两个常用的激活函数的计算图。

#### Sigmoid层

Sigmoid函数的形式如下：
$$\sigma(x) = \frac{1}{1+e^{-x}}$$

我们根据公式绘制出计算图：

![](https://files.mdnice.com/user/70350/b854a6e9-834e-48eb-829b-b565572f218f.png)

图中除了加法节点，乘法节点以外，还出现了新的"exp"节点和除法节点。

接下来，我们就分步推导出Sigmoid的反向传播过程。

**第一步**

"/"（除法）节点表示$\sigma = \frac{1}{x}$,那么它的导数为：
$$\frac{\partial \sigma}{\partial x} = - \frac{1}{x^2} = -\sigma^2$$

那么除法节点在反向传播时，会将上游的值乘以$- \frac{1}{x^2}$后，再传递给下游。如图：

![](https://files.mdnice.com/user/70350/7335524c-d670-4fcb-89af-0e1dc771be42.png)



**第二步**
加法节点会将上游的值原封不动的传递给下游。

**第三步**
"exp"节点表示$y=e^x$,它的导数为：
$$\frac{\partial y}{\partial x} = e^x$$

所以在计算图中，上游的值经过"exp"节点，需要乘上正向传播时这个节点的输出(这里是$e^{-x}$)，再传递给下游。

![](https://files.mdnice.com/user/70350/d6aac9ff-59ec-425a-8747-69a6b3afde3d.png)



**第四步**
乘法节点会将上游值乘以正向传播时反面的值再传递给下游。所以这里要乘以-1。

![](https://files.mdnice.com/user/70350/1d61a896-a55f-4da8-99a5-f24ef698c8f9.png)



**最后一步**

经过整理，Sigmoid函数在计算图中的反向传播输出为：
$$\frac{\partial L}{\partial \sigma} \cdot -\sigma^2 \cdot e^{-x}  \cdot -1 =\frac{\partial L}{\partial \sigma} \cdot \sigma^2 \cdot e^{-x} = \frac{\partial L}{\partial \sigma} \cdot \frac{1}{(1+e^{-x})^2} \cdot e^{-x} =  \frac{\partial L}{\partial \sigma} \cdot \frac{1}{1+e^{-x}} \cdot \frac{e^{-x}}{1+e^{-x}} =  \frac{\partial L}{\partial \sigma} \cdot \sigma \cdot (1-\sigma)$$

将计算图化解后如下：

![](https://files.mdnice.com/user/70350/1881b888-066c-45be-891f-86f1c13cb43c.png)


代码实现：
```python
class sigmoid:
  def __init__(self):
    self.out = None
  
  def forward(self,x):
    self.out = 1/(1+np.exp(-x))
    return out
    
  def backward(self,dout):
    dx = dout * (1.0 - self.out) * self.out
    return dx
```

#### ReLU层

激活函数除了常用的Sigmoid以外，还有ReLU函数。ReLU(Rectified Linear Unit，线性整流函数)是一种常用的激活函数，广泛应用于深度学习中，尤其是卷积神经网络（CNN）中。它主要特点是，计算简单，收敛速度快，并且在实际应用中效果显著。

其定义如下：

$$ReLU(z) = max(0,z)$$ 

或

$$\sigma =  \begin{cases} z &(z>0)\\ 0 &(z \leq 0)\end{cases}$$

求$sigma$关于z的导数如下：

$$\frac{\partial \sigma}{\partial z} = \begin{cases} 1 & (z > 0) \\ 0 & (z \leq 0)\end{cases}$$

从上式可以看出，如果正向传播时输入z大于0，则反向传播时会将上游值原封不动的传递给下游。相反，如果正向传播时输入z小于等于0,反向传播时则不往下游传递数据。

计算图如下：
![](https://files.mdnice.com/user/70350/0d6189dd-eb8e-426a-bf51-d6c0be8d960e.png)
![](https://files.mdnice.com/user/70350/22850b48-64f3-47c7-a02c-d5aa237d2a1a.png)

代码实现：
```python
 class Relu:
   def __init__(self):
     self.mask = None
  
   def forward(self,x):
     self.mask = (x <= 0)
     out = x.copy()
     out[self.mask] = 0
     
     return out
    
   def backward(self,dout):
     dout[self.mask] = 0
     dx =dout
     
     return dx
```
在代码中我们定义了一个mask变量，该变量用于保存前向传播时哪些位置的值小于0，如果该位置的输入数据小于0，则会将该位置设置为True。
例如：输入数据为[0.1,0.2,-0.8] 则mask的值为[False,False,True]。

在前向传播时，我们将输入值小于0的都设置为0，并且记录了小于0的位置。在反向传播时我们再使用mask将前向传播时设置为0的地方，其梯度也设置为0。因为该位置在前向传播时就没有传递信息，所以它对损失函数的变化没有任何的贡献。


### Softmax-with-Loss 层

输出层使用的激活函数比较特殊，在之前的小节中我们一直使用的Softmax函数，并且使用交叉熵损失函数与其搭配使用。所以在计算图里，我们将它们放在同一层。

Softmax函数与交叉熵函数的计算图推理比较烦琐，这里就不详细讲解推理步骤了，直接给出简化后的图。

![](https://files.mdnice.com/user/70350/d32ca030-b253-4404-88f2-6b92027a528f.png)

这里的t是真实的标签。最后反向传播时得到的结果就是输出值$\sigma$减去真实标签t。

代码实现：
```python
  import numpy as np
  
  def cross_entropy_error(y_true,y_pred):
    # 使用np.log计算对数时，避免出现log(0)的情况
    y_pred = np.clip(y_pred,1e-7,1 - 1e-7)

    loss = - np.sum(y_true * np.log(y_pred))

    return loss
  
  def softmax(z):
   exp_z = np.exp(z)
   sum = np.sum(exp_z)
   out  = exp_z / sum
   
   return out
  
  class SoftmaxWithLoss:
    def __init__(self):
      #损失
      self.loss = None
      #输出
      self.sigma = None
      #标签
      self.t = None
    
    def forward(self,x,t):
      self.t = t
      self.sigma = softmax(x)
      self.loss = cross_entropy_error(self.sigma,self.t)
      
      return self.loss
    
    def backward(self,dout = 1):
      dx = self.sigma - self.t
      
      return dx
```

## 总结

本小节我们引入了计算图，通过计算图的视角来理解误差反向传播。可以看见使用计算图的方式能让我们更清晰的观察到误差反向传播的过程，并且在实际写代码时让代码更容易模块化。未来在构建神经网络时只需要以层的方式进行组装即可。