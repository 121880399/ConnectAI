## 前文提要
上一小节我们讲述了神经网络中层次分布，并且讲述了如何通过输入数据和权重计算隐藏层的输入数据，最后将输入数据给到激活函数进行非线性运算。

## 一个简单的例子

由于上一小节的内容过于理论化，这一小节咱们先来看一个具体的例子。

![](https://files.mdnice.com/user/70350/303bb4cb-f469-4417-9010-389dc5611421.png)


上图中显示了一个神经网络，只有输入层和输出层。现在我们分别计算一下隐藏层各节点的输入数据是多少？

$z_2^1 = 0.5*0.9 + 0.1*0.4 + 0.4*0.6 = 0.73$ 

$z_2^2 = 0.5*0.5 + 0.1*0.3 + 0.4*0.7 = 0.56$  

$z_2^3 = 0.5*0.2 + 0.1*0.1 + 0.4*0.4 = 0.27$  

以上就是隐藏层各节点的输入值，把这些输入值加上偏置给激活函数就能得到输出值。

## 矩阵

上面的例子很简单，每层都只有3个节点。如果隐藏层100个节点怎么办？如果输入层有3个节点，隐藏层有100个节点，那么权重就有300个。如果还按照上面那样计算，会非常繁琐。那么有什么办法可以简化一下呢？答案就是矩阵。

我们可以将输入数据当作一个3行1列的矩阵，上面的例子就是：

$x$ = 
$\begin{pmatrix}
0.5 \\ 
0.1 \\
0.4\\
\end{pmatrix}$

而将权重也当作一个矩阵:

$w=\\$
$\begin{pmatrix}
{w_1}_1 & {w_1}_2 & {w_1}_3 \\ 
{w_2}_1 & {w_2}_2 & {w_2}_3 \\
{w_3}_1 & {w_3}_2 & {w_3}_3 \\
\end{pmatrix}$
$=\\$
$\begin{pmatrix}
0.9 & 0.4 & 0.6 \\ 
0.5 & 0.3 & 0.7 \\
0.2 & 0.1 & 0.4 \\
\end{pmatrix}$

${w_i}_j$ 就表示前一层第j个节点到本层第i个节点的权重。

例如：

${w_2}_1$就表示输入层第1个节点到隐藏层第2个节点的权重。

权重矩阵的形状(m,n)，m表示后一层节点数量，n表示前一层节点的数量。

## 关于矩阵的小知识

这里补充几个关于矩阵的小知识，以后可能会用到。

1.方阵

当矩阵的行数等于列数，那么称为方阵。

例如：

$\begin{pmatrix}
0.9 & 0.4 & 0.6 \\ 
0.5 & 0.3 & 0.7 \\
0.2 & 0.1 & 0.4 \\
\end{pmatrix}$

2.单位矩阵

对角线上的元素都为1，其他元素为0的方阵称为单位矩阵，用E表示。

$E=\\$
$\begin{pmatrix}
1 & 0 & 0 \\ 
0 & 1 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}$

3.矩阵相等

如果两个矩阵的对应元素都相等，称两个矩阵相等。

例如：

$A=\\$
$\begin{pmatrix}
2 &5  \\ 
3 & 7  \\
\end{pmatrix}$

$B=\\$
$\begin{pmatrix}
2 &5  \\ 
3 & 7  \\
\end{pmatrix}$

我们可以说矩阵$A$ = $B$。


4.转置矩阵

将矩阵$A$的第$i$行第$j$列元素与第$j$行第$i$列元素交换，由此产生的矩阵称为矩阵$A$的转置矩阵，用$A^t$表示。

例如：

$A=\\$
$\begin{pmatrix}
2 &5  \\ 
3 & 7  \\
\end{pmatrix}$

$A^t=\\$
$\begin{pmatrix}
2 &3  \\ 
5 & 7  \\
\end{pmatrix}$


$B=\\$
$\begin{pmatrix}
2   \\ 
3   \\
\end{pmatrix}$

$B^t=${2 3}



$C=\\$
$\begin{pmatrix}
2 &5 & 1 \\ 
3 & 7 & 4 \\
\end{pmatrix}$

$C^t=\\$
$\begin{pmatrix}
2 &3  \\ 
5 & 7  \\
1 & 4  \\
\end{pmatrix}$

## 矩阵乘法

我们现在知道了关于矩阵的知识，那么在神经网络中矩阵是如何运用的呢？这里就需要用到矩阵的乘法。还是用上面的例子。

$x$ =$\begin{pmatrix}
0.5 \\ 
0.1 \\
0.4\\
\end{pmatrix}$

$w=\\$
$\begin{pmatrix}
0.9 & 0.4 & 0.6 \\ 
0.5 & 0.3 & 0.7 \\
0.2 & 0.1 & 0.4 \\
\end{pmatrix}$

$w · x=\\$
$\begin{pmatrix}
0.9 & 0.4 & 0.6 \\ 
0.5 & 0.3 & 0.7 \\
0.2 & 0.1 & 0.4 \\
\end{pmatrix}\\$
·
$\begin{pmatrix}
0.5 \\ 
0.1 \\
0.4\\
\end{pmatrix}$
= $\begin{pmatrix}
0.73 \\ 
0.56 \\
0.27\\
\end{pmatrix}$

![](https://files.mdnice.com/user/70350/0e82a5d8-90cb-481b-af99-55954b9df32a.png)




这里将$w$的第i行向量与$x$的第j列的内积作为矩阵$w · x$的第i行第j列的元素。

例如$w$的第1行与$x$的第1列的内积作为矩阵$w · x$第1行第1列的元素。


这样我们就将之前的加权求和转换成了矩阵乘法。



这里有两点需要注意：

1.矩阵乘法必须满足，矩阵A的列数跟矩阵B的行数相等。例如以上例子中：

![](https://files.mdnice.com/user/70350/99153bb0-7cc6-4e20-b9cf-f830e6f8b47e.png)

输入数据$x$的形状是（3,1）3行1列，权重矩阵$w$的形状是（3,3）3行3列，$x$的行与$w$的列数相等。输出的结果为$w$的行数$x$的列数。

2.矩阵乘法不满足交换律。

例如：

$A=\\$
$\begin{matrix}
2 &5  \\ 
3 & 7  \\
\end{matrix}$

$B=\\$
$\begin{matrix}
2 &1  \\ 
3 & 3  \\
\end{matrix}$

$A · B = \\$
$=\begin{matrix}
19 &17  \\ 
27 & 24  \\
\end{matrix}\\$

$B · A = \\$
$=\begin{matrix}
7 &17  \\ 
15 & 36  \\
\end{matrix}\\$

$A · B \neq B · A$

所以大家可以看见在上面的例子中我们使用的是权重$w$乘以输入数据$x$，而不是输入数据$x$乘以$w$。这主要是因为输入数据大概率是(n,1)的形状，为了满足矩阵A的列数跟矩阵B的行数相等，我们将权重放在前面。

接下来我们看一下代码实现：
```python
import numpy as np

x = np.array([[0.5],[0.1],[0.4]])
print(x.shape)

w = np.array([[0.9,0.4,0.6],[0.5,0.3,0.7],[0.2,0.1,0.4]])
print(w.shape)

print(np.dot(w,x))
```

输出结果如下：

```
(3,1)
(3, 3)
[[0.73]
 [0.56]
 [0.27]]
```
代码中我们使用numpy这个库。

NumPy（Numerical Python）是一个用于科学计算的基础库，以其高效的数组和矩阵计算能力闻名。它提供了支持大规模多维数组与矩阵的运算功能，以及大量的数学函数库，用于执行这些数组上的操作。

我们这里使用了array方法。该函数能够将输入的数据（例如列表、元组等）转换为 NumPy 数组。

如何我们用shape函数打印出了$x$和$w$的形状。$x$为二维矩阵，有3行1列。$w$为二维矩阵，有3行3列。

最后我们使用dot函数来执行矩阵的乘法。

## 总结

在本小节中，我们首先通过一个简单的例子回顾了隐藏层输入数据的算法。接着我们介绍了矩阵已经矩阵的乘法，最后我们通过矩阵的乘法来简化隐藏层输入数据的计算。